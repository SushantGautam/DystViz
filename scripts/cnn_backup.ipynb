{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mubl\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "e:\\MSCProjects\\mobile-gaitlab\\.venv\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\DystoniaCoalition\\scripts\\wandb\\run-20220320_215716-lnyg96oy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ubl/DystoniaColiation/runs/lnyg96oy\" target=\"_blank\">Random Score</a></strong> to <a href=\"https://wandb.ai/ubl/DystoniaColiation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"E:\\\\MSCProjects\\\\mobile-gaitlab\\\\training\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "from tensorflow.keras import backend as K\n",
    "tf.compat.v1.set_random_seed(1)\n",
    "#sess = tf.Session(graph=tf.get_default_graph())\n",
    "#K.set_session(sess)\n",
    "import sys \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow.keras.regularizers\n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import pickle\n",
    "from video_process_utils import *\n",
    "import collections, re\n",
    "import wandb\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "experiment_name=\"Random Score\"\n",
    "wandb.init(project=\"DystoniaColiation\", entity=\"ubl\", name=experiment_name)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df_fixed8 = pd.read_hdf(\"./norm_df_fixed8.hdf\", key='han')\n",
    "dataForLoop = norm_df_fixed8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDRS= pd.read_excel(\"D:\\DystoniaCoalition\\dataset\\P1.LV.Koirala.2021.3.30-2_edited.xlsx\", sheet_name='GDRS', engine='openpyxl')\n",
    "GDRS.columns\n",
    "GDRS.drop(GDRS.columns[[1,2,3,4]],axis=1,inplace=True)\n",
    "GDRS.rename(columns={ GDRS.columns[0]: \"PID\" }, inplace = True)\n",
    "GDRS = GDRS.loc[:, ~GDRS.columns.str.startswith('Unnamed')].iloc[1:].reset_index(drop=True)\n",
    "GDRS['PID'] = GDRS['PID'].apply(lambda x: re.findall(\"\\d+\", x)[0])\n",
    "\n",
    "scores = GDRS[GDRS['PID'].isin(list(norm_df_fixed8[('Dystonia', 'PID')].unique().astype(str)))].iloc[:,5:6] #4:6\n",
    "neck_scores = scores.values.reshape(-1).tolist()\n",
    "neck_scores=  neck_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLip X axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "clone  = norm_df_fixed8.copy()\n",
    "clone.update(clone.loc[:,  clone.columns.get_level_values(1).isin(['x'])].apply(lambda x: -x))\n",
    "clone[('Dystonia', \"PID\")] = clone[('Dystonia', \"PID\")].apply(lambda x: -x,)\n",
    "norm_df_fixed8_Withflipped = norm_df_fixed8.append(clone)\n",
    "dataForLoop = norm_df_fixed8_Withflipped\n",
    "\n",
    "neck_scores_Withflipped =  neck_scores\n",
    "neck_scores_Withflipped.extend(neck_scores)\n",
    "y= neck_scores_Withflipped\n",
    "\n",
    "# y= [random.randint(0,9) for i in range(len(y))]  #ramdom score to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for name, _df in dataForLoop.groupby(('Dystonia','PID')):\n",
    "    eachVideo = []\n",
    "    for _name, row in _df.iterrows():\n",
    "        eachVideo.append(list(row.iloc[2:18]))\n",
    "    X.append(eachVideo)\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.00, stratify=y, random_state=42)\n",
    "# pd.Series(list(y_train)).value_counts()\n",
    "\n",
    "X_train = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "X_validation = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "y_validation = tf.convert_to_tensor(y, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_range=10\n",
    "target_column=\"Neck\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(initial_lrate,epochs_drop,drop_factor):\n",
    "    def step_decay_fcn(epoch):\n",
    "        return max(initial_lrate * math.pow(drop_factor, math.floor((1+epoch)/epochs_drop)), 0.002)\n",
    "    return step_decay_fcn\n",
    "\n",
    "epochs_drop,drop_factor = (20,0.8)\n",
    "initial_lrate = 0.01\n",
    "dropout_amount = 0.1  # overfit in training\n",
    "last_layer_dim = 10\n",
    "filter_length = 8\n",
    "conv_dim = 32\n",
    "l2_lambda = 10**(-3.5)\n",
    "\n",
    "def w_mse(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        #multiply by len(weights) to make the magnitude invariant to number of components in target\n",
    "        return K.mean(K.sum(K.square(y_true-y_pred)*weights,axis=1)*tf.reshape(y_true[:,-1],(-1,1)))\n",
    "    return loss\n",
    "\n",
    "#we don't want to optimize for the column counting video occurences of course, but\n",
    "#they are included in the target so we can use that column for the loss function\n",
    "weights = [1.0]\n",
    "normal_weights = [1.0]\n",
    "\n",
    "\n",
    "#normalize weights to sum to 1 to prevent affecting loss function\n",
    "weights = weights/np.sum(weights)\n",
    "normal_weights = normal_weights/np.sum(normal_weights)\n",
    "\n",
    "mse_opt = w_mse(weights)\n",
    "\n",
    "#monitor our actual objective\n",
    "mse_metric = w_mse(target_range**2*normal_weights)\n",
    "\n",
    "hyper_str = \"params_\"\n",
    "for param in [initial_lrate,epochs_drop,drop_factor,dropout_amount,conv_dim,last_layer_dim,filter_length,l2_lambda]:\n",
    "    hyper_str = hyper_str + str(param) + \"_\"\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "wandb.config.update({\n",
    "\"epochs\": n_epochs,\n",
    "\"epochs_drop,drop_factor\" : (epochs_drop,drop_factor),\n",
    "\"initial_lrate\" : initial_lrate,\n",
    "\"dropout_amount\" : dropout_amount,\n",
    "\"last_layer_dim\" : last_layer_dim,\n",
    "\"filter_length\" : filter_length,\n",
    "\"conv_dim\" : conv_dim,\n",
    "\"l2_lambda\" : l2_lambda,\n",
    "})\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_session(sess)\n",
    "\n",
    "model = Sequential(name=\"Neck_model\")\n",
    "model.add(Conv1D(conv_dim,filter_length ,padding='same')) # input_dim=X_train.shape[2],input_length=vid_length\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(conv_dim,filter_length,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Conv1D(conv_dim,filter_length,padding='same',kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(conv_dim,filter_length,padding='same',kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Conv1D(conv_dim,filter_length,padding='same',kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(conv_dim,filter_length,padding='same',kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(last_layer_dim,activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 509.7854 - val_loss: 1193.5706\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 783.6105 - val_loss: 284.1475\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 256.8203 - val_loss: 130.2187\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 130.6626 - val_loss: 124.8750\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 126.0093 - val_loss: 108.6517\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 186.7793 - val_loss: 243.6836\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 191.9925 - val_loss: 75.9501\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 114.9955 - val_loss: 54.8665\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 121.6227 - val_loss: 129.6591\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 148.1480 - val_loss: 153.1437\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 125.2775 - val_loss: 192.9897\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 210.0259 - val_loss: 193.2534\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 191.2121 - val_loss: 83.9767\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 125.9845 - val_loss: 88.9436\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 128.4695 - val_loss: 82.7190\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 111.6185 - val_loss: 71.1144\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 158.1268 - val_loss: 114.6734\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 123.7700 - val_loss: 117.9304\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 161.2442 - val_loss: 145.9179\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 130.2894 - val_loss: 56.7905\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 90.0185 - val_loss: 46.0369\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 77.2756 - val_loss: 56.6151\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 76.1196 - val_loss: 42.3419\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 78.5483 - val_loss: 48.8421\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 86.7913 - val_loss: 100.2580\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 121.7888 - val_loss: 70.6159\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 110.9772 - val_loss: 125.4053\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 141.4867 - val_loss: 133.0626\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 146.4415 - val_loss: 48.2960\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 83.7119 - val_loss: 61.2558\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 85.8830 - val_loss: 69.6898\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 105.6129 - val_loss: 56.9165\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 81.9701 - val_loss: 54.5854\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 72.2397 - val_loss: 54.9077\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 105.5688 - val_loss: 103.3768\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 123.7415 - val_loss: 144.4626\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 124.0628 - val_loss: 61.1173\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 87.9506 - val_loss: 47.4018\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 84.0886 - val_loss: 50.0206\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 91.4774 - val_loss: 112.1243\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 96.7518 - val_loss: 60.5642\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 66.7794 - val_loss: 44.4370\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 77.4780 - val_loss: 70.4290\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 80.9763 - val_loss: 65.7061\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 64.9054 - val_loss: 65.3967\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 79.1052 - val_loss: 44.3317\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 82.0949 - val_loss: 74.5303\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 116.7472 - val_loss: 73.8101\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 90.6548 - val_loss: 48.5129\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 64.7587 - val_loss: 62.9206\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 66.2783 - val_loss: 42.2051\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 71.2013 - val_loss: 57.6830\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 68.2405 - val_loss: 62.8067\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 67.0988 - val_loss: 75.6624\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 67.7932 - val_loss: 57.0623\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 95.9593 - val_loss: 128.8764\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 116.9888 - val_loss: 69.1752\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 75.6996 - val_loss: 86.2410\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 85.2008 - val_loss: 56.3502\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 73.2750 - val_loss: 41.4484\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 56.1872 - val_loss: 36.7370\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 56.1835 - val_loss: 49.9759\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 62.3000 - val_loss: 76.4439\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 61.3860 - val_loss: 68.2785\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 68.2636 - val_loss: 60.3202\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 57.7450 - val_loss: 52.7861\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 54.2831 - val_loss: 68.0009\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 62.8408 - val_loss: 88.3459\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 63.7055 - val_loss: 64.7327\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 245ms/step - loss: 60.0980 - val_loss: 75.9563\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 205ms/step - loss: 68.6742 - val_loss: 77.3060\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 235ms/step - loss: 72.7660 - val_loss: 94.1491\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 201ms/step - loss: 80.9974 - val_loss: 50.3343\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 60.1087 - val_loss: 69.1906\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 201ms/step - loss: 89.1392 - val_loss: 95.5358\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 236ms/step - loss: 68.3265 - val_loss: 51.3319\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 63.8944 - val_loss: 48.2436\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 65.9446 - val_loss: 76.1275\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 56.0663 - val_loss: 54.7869\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 53.3257 - val_loss: 50.4753\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 54.4211 - val_loss: 57.0539\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 50.9532 - val_loss: 42.7679\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 58.8112 - val_loss: 45.2038\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 55.9947 - val_loss: 63.2784\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 51.2432 - val_loss: 61.7625\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 54.5165 - val_loss: 55.7857\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 48.1269 - val_loss: 42.2309\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 56.7472 - val_loss: 47.3593\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 45.1119 - val_loss: 68.3037\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 59.7566 - val_loss: 89.9171\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 69.5751 - val_loss: 65.4798\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 57.6641 - val_loss: 46.5249\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 53.8560 - val_loss: 54.1527\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 53.5746 - val_loss: 60.1071\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 58.8099 - val_loss: 89.1555\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 71.1588 - val_loss: 61.8677\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 57.3670 - val_loss: 60.9239\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 53.3115 - val_loss: 44.0746\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 48.1054 - val_loss: 55.1934\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 51.9412 - val_loss: 53.7389\n"
     ]
    }
   ],
   "source": [
    "opt = RMSprop(lr=0.0,rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=mse_opt,metrics=[mse_metric],\n",
    "                optimizer=opt)\n",
    "\n",
    "lr = LearningRateScheduler(step_decay(initial_lrate,epochs_drop,drop_factor))\n",
    "\n",
    "history = model.fit(X_train, y_train,callbacks=[\n",
    "    lr,\n",
    "    TerminateOnNaN(), WandbCallback()],\n",
    "            validation_data=(X_validation,y_validation),\n",
    "            batch_size=400, epochs=n_epochs,\n",
    "            shuffle=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18c327b35c8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    'PID': [name for name, _df in dataForLoop.groupby(('Dystonia','PID'))],\n",
    "    'Prediction': list(model.predict(X_train).flatten()),\n",
    "    'Original': list(y_train.numpy())\n",
    "}\n",
    "\n",
    "result = pd.DataFrame(my_dict)\n",
    "\n",
    "result['diff'] = np.abs(result['Original'] - result['Prediction'])\n",
    "result['Prediction'] = np.abs(result['Prediction'].astype(int))\n",
    "\n",
    "\n",
    "result.sort_values(by=['diff']).head(60)\n",
    "# result\n",
    "# list(y_train.numpy())\n",
    "\n",
    "result['diff'].plot(kind='hist',bins=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ea23dd9d6fa550749aa0b9c2e2536f45c10b0b6f434477bdb3c4ff4311cba66"
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
